changestream {
  mysql {
    host = "localhost"
    port = 3306
    user = "root"
    password = "password"

    // Changestream must have a server-id that is unique in the cluster
    server-id = 93

    // Timeout for binlog client and column info queries
    timeout = 5000 #ms

    // Keepalive interval for binlog client
    keepalive = 5000 #ms

    // Preload settings for the column info actor
    preload {
      // Preload specific databases (optional)
      databases = "" #comma separated

      // The timeout for the column info query should be longer than other timeouts,
      // since we are querying the INFORMATION_SCHEMA
      timeout = 30000 #ms
    }

    host = ${?MYSQL_HOST}
    port = ${?MYSQL_PORT}
    user = ${?MYSQL_USER}
    password = ${?MYSQL_PASS}
    server-id = ${?MYSQL_SERVER_ID}
    timeout = ${?MYSQL_TIMEOUT}
    keepalive = ${?MYSQL_KEEPALIVE}
    preload.databases = ${?MYSQL_PRELOAD_DATABASES}
    preload.timeout = ${?MYSQL_PRELOAD_TIMEOUT}
  }

  // Fully qualified class path of the actor to use for emitting events
  // Default:
  //   emitter = "changestream.actors.StdoutActor"
  emitter = ${?EMITTER}

  // Include any database or table identifiers that you'd like to include.
  // This is an explicit list so no other db/table combinations will be included.
  // For example:
  //   allowlist = "my_app.*,my_other_app.customers"
  // Or provide them as ENVs
  allowlist = ${?ALLOWLIST}

  // Exclude database or table identifiers. The allowlist supercedes the blocklist setting.
  // Note, the following databases are always ignored:
  //   - information_schema
  //   - mysql
  //   - performance_schema
  //   - sys
  //
  // For example:
  //   blocklist = "my_other_app.transactions,my_other_app.other_stuff"
  // Or provide them as ENVs
  blocklist = ${?BLOCKLIST}

  // Whether or not to truncate the SQL. This can be useful when you would like to have the SQL (by enabling ROWS_QUERY
  // events in the binlog), but your application produces very large SQL queries, which can consume excessive memory in
  // changestream's JsonFormatterActor. Set a limit on SQL to avoid OOM issues with these large SQL queries.
  //   0 = no truncation
  //   n = emit a max of n characters of SQL
  sql-character-limit = 0
  sql-character-limit = ${?SQL_CHARACTER_LIMIT}

  // Whether or not to include the row data
  include-data = true
  include-data = ${?INCLUDE_DATA}

  // Pretty print the JSON?
  pretty-print = true
  pretty-print = ${?PRETTY_PRINT}

  // To create a key (scala console):
  //   val keygen = javax.crypto.KeyGenerator.getInstance("AES")
  //   keygen.init(128) // note doesn't work in 256 yet
  //   java.util.Base64.getEncoder.encodeToString(keygen.generateKey().getEncoded)
  encryptor {
    enabled = false
    encrypt-fields = "row_data,old_row_data,query.sql"
    timeout = 500 #ms
    cipher = "AES"
    key = "yRVCuDlryZdWhSUDwLNugQ=="

    enabled = ${?ENCRYPTOR_ENABLED}
    encrypt-fields = ${?ENCRYPTOR_ENCRYPT_FIELDS}
    timeout = ${?ENCRYPTOR_TIMEOUT}
    cipher = ${?ENCRYPTOR_CIPHER}
    key = ${?ENCRYPTOR_KEY}
  }

  aws {
    // AWS Region to use for AWS Emitters
    region = "us-east-1"

    // The SQS queue to which events should be sent. If the queue does not exist, it will be created.
    sqs.queue = "firehose"

    // Optionally interpolate database name and/or table name in topic, for example:
    //   "{database}-firehose"
    //   "{tableName}-firehose"
    //   "{database}.{tableName}-firehose"
    // Note: String interpolation is only supported in SNS at this time
    sns.topic = "firehose"

    s3 {
      // Make sure this directory is secure (trailing forward-slash is important).
      // Defaults to empty string, which means use the default system temp dir.
      // The directory should exist and be writeable by the changestream user.
      buffer-temp-dir = ""
      // The bucket name that is the destination for change events
      bucket = ""
      // Key prefix to use (e.g. production_db/)
      key-prefix = ""
      // Maximum number of changes to buffer before writing to S3
      batch-size = 1000 # ct messages
      // Maximum amount of time before flushing change buffer to S3
      flush-timeout = 300000 # ms

      buffer-temp-dir = ${?AWS_S3_BUFFER_TEMP_DIR}
      bucket = ${?AWS_S3_BUCKET}
      key-prefix = ${?AWS_S3_KEY_PREFIX}
      batch-size = ${?AWS_S3_BATCH_SIZE}
      flush-timeout = ${?AWS_S3_FLUSH_TIMEOUT}
    }

    // Timeout for requests to AWS
    timeout = 5000 #ms

    sqs.queue = ${?AWS_SQS_QUEUE_NAME}
    sns.topic = ${?AWS_SNS_TOPIC_NAME}
    timeout = ${?AWS_TIMEOUT}
    region = ${?AWS_REGION}
  }

  // Http status and control server
  control.host = 127.0.0.1
  control.host = ${?CONTROL_HOST}
  control.port = 9521
  control.port = ${?CONTROL_PORT}

  //   position-saver.actor = "changestream.actors.PositionSaver"
  position-saver.actor = ${?POSITION_SAVER_ACTOR}
  position-saver.file-path = "/tmp/changestream.pos"
  position-saver.file-path = ${?POSITION_SAVER_FILE_PATH}
  position-saver.max-records = 10
  position-saver.max-records = ${?POSITION_SAVER_MAX_RECORDS}
  position-saver.max-wait = 1000
  position-saver.max-wait = ${?POSITION_SAVER_MAX_WAIT}

  // Limit the number of in flight messages -- this can prevent unpredictable behavior due to memory pressure when
  // there is a spike of write activity in MySQL. This acts as a very basic flow control for the system. Tune this
  // number based on the available heap size in your implementation.
  // Set to zero to disable.
  in-flight-limit = 0
  in-flight-limit = ${?CHANGESTREAM_IN_FLIGHT_LIMIT}
}

akka {
  coordinated-shutdown.exit-jvm = on

  # disconnect from binlog client
  coordinated-shutdown.phases.before-service-unbind.timeout = 6s
  coordinated-shutdown.phases.before-service-unbind.recover = on
  # kill the event pipeline
  coordinated-shutdown.phases.service-unbind.timeout = 31s
  coordinated-shutdown.phases.service-unbind.recover = on
  # drain emitter
  coordinated-shutdown.phases.service-requests-done.timeout = 31s
  coordinated-shutdown.phases.service-requests-done.recover = on
  # shut down control server
  coordinated-shutdown.phases.service-stop.timeout = 6s
  coordinated-shutdown.phases.service-stop.recover = on

  actor.guardian-supervisor-strategy = "changestream.ChangestreamSupervisorStrategy"

  loggers = ["akka.event.slf4j.Slf4jLogger"]
  logging-filter = "akka.event.slf4j.Slf4jLoggingFilter"
  log-dead-letters = 10
  log-dead-letters-during-shutdown = off

  loglevel = "INFO"
  loglevel = ${?AKKA_LOG_LEVEL}
}

kamon {
  // Configure your reporters here. Datadog and Prometheus modules have been packaged into docker for convenience
  //reporters = ["kamon.prometheus.PrometheusReporter"]
  reporters = [${?KAMON_REPORTER_1}, ${?KAMON_REPORTER_2}, ${?KAMON_REPORTER_3}, ${?KAMON_REPORTER_4}]

  metric.tick-interval = ${?KAMON_TICK_INTERVAL}

  # https://github.com/kamon-io/kamon-prometheus
  environment {
    # Identifier for this service.
    service = "changestream"
    service = ${?KAMON_SERVICE}

    # Identifier for the host where this service is running. If set to `auto` Kamon will resolve the hostname using
    # the resolved name for localhost.
    host = ${?KAMON_HOST}

    # Identifier for a particular instance of this service. If set to `auto` Kamon will use the pattern service@host.
    instance = ${?KAMON_INSTANCE}

    tags {
      env = development
      env = ${?KAMON_ENV}
    }
  }

  datadog {
    agent.hostname = ${?DATADOG_HOSTNAME}
    agent.port = ${?DATADOG_PORT}
    http.api-key = ${?DATADOG_API_KEY}
    flush-interval = ${?DATADOG_FLUSH_INTERVAL}
    max-packet-size = ${?DATADOG_MAX_PACKET_SIZE}
  }

  newrelic {
    nr-insights-insert-key = ${?NEW_RELIC_API_KEY}
  }

  prometheus {
    include-environment-tags = yes

    embedded-server {
      hostname = 0.0.0.0
      port = 9095

      hostname = ${?PROMETHEUS_HOSTNAME}
      port = ${?PROMETHEUS_PORT}
    }
  }

  util.filters {
    "akka.tracked-actor" {
      includes = [
        "changestream/user/positionSaverActor",
        "changestream/user/emitterActor",
        "changestream/user/transactionActor",
        "changestream/user/columnInfoActor",
        "changestream/user/formatterActor"
      ]
    }

    "akka.traced-actor" {
      includes = [
        "changestream/user/positionSaverActor",
        "changestream/user/emitterActor",
        "changestream/user/transactionActor",
        "changestream/user/columnInfoActor",
        "changestream/user/formatterActor"
      ]
    }
  }
}

include "application.overrides"
